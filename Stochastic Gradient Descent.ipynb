{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "To adjust the weights and biases using the derivatives gathered from back propagtion, Stochastic Gradient Descent will be used. Three hyperparemters will be used to ensure the global minimum will be reached: a learning rate, a learning rate decay and momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relavant modules\n",
    "import numpy as np\n",
    "\n",
    "# Import class objects for the neural network and spiral data\n",
    "from layerdense import *\n",
    "from costfunctions import *\n",
    "from spiraldata import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimiser class\n",
    "class Optimizer_SGD:\n",
    "    # Initialise optimiser and set default params\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0.0, momentum = 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay \n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        # If decay argument, apply decay to current learning rate calculations\n",
    "        self.current_learning_rate = self.learning_rate * \\\n",
    "                                     (1.0 / (1.0 + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If momentum was passed\n",
    "        if self.momentum:\n",
    "            # If layer output does not contain momentum arrays\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "                weight_updates = \\\n",
    "                                 self.momentum * layer.weight_momentums - \\\n",
    "                                 self.current_learning_rate * layer.dweights\n",
    "                layer.weight_momentums = weight_updates\n",
    "                \n",
    "                # Update bias\n",
    "                bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                               self.current_learning_rate * layer.dbiases\n",
    "                layer.bias_momentums = bias_updates\n",
    "                \n",
    "                # If no momentum is used \n",
    "            else:\n",
    "                weight_updates = -self.current_learning_rate * \\\n",
    "                                  layer.dweights\n",
    "                bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "                \n",
    "            # Update weights and biases\n",
    "            layer.weights += weight_updates\n",
    "            layer.biases += bias_updates\n",
    "        \n",
    "    # Call once after a params update\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the optimising function using the spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.283, loss: 1.099, lr: 1.0\n",
      "epoch: 500, acc: 0.437, loss: 1.061, lr: 0.66711140760507\n",
      "epoch: 1000, acc: 0.433, loss: 1.052, lr: 0.5002501250625312\n",
      "epoch: 1500, acc: 0.467, loss: 1.018, lr: 0.4001600640256102\n",
      "epoch: 2000, acc: 0.510, loss: 0.980, lr: 0.33344448149383127\n",
      "epoch: 2500, acc: 0.480, loss: 0.956, lr: 0.2857959416976279\n",
      "epoch: 3000, acc: 0.513, loss: 0.933, lr: 0.25006251562890724\n",
      "epoch: 3500, acc: 0.543, loss: 0.902, lr: 0.22227161591464767\n",
      "epoch: 4000, acc: 0.547, loss: 0.875, lr: 0.2000400080016003\n",
      "epoch: 4500, acc: 0.580, loss: 0.847, lr: 0.18185124568103292\n",
      "epoch: 5000, acc: 0.603, loss: 0.823, lr: 0.16669444907484582\n",
      "epoch: 5500, acc: 0.630, loss: 0.798, lr: 0.15386982612709646\n",
      "epoch: 6000, acc: 0.643, loss: 0.773, lr: 0.1428775539362766\n",
      "epoch: 6500, acc: 0.650, loss: 0.750, lr: 0.13335111348179757\n",
      "epoch: 7000, acc: 0.667, loss: 0.726, lr: 0.12501562695336915\n",
      "epoch: 7500, acc: 0.700, loss: 0.702, lr: 0.11766090128250381\n",
      "epoch: 8000, acc: 0.703, loss: 0.682, lr: 0.11112345816201799\n",
      "epoch: 8500, acc: 0.710, loss: 0.666, lr: 0.10527423939362038\n",
      "epoch: 9000, acc: 0.727, loss: 0.648, lr: 0.1000100010001\n",
      "epoch: 9500, acc: 0.717, loss: 0.634, lr: 0.09524716639679968\n",
      "epoch: 10000, acc: 0.720, loss: 0.624, lr: 0.09091735612328393\n",
      "epoch: 10500, acc: 0.727, loss: 0.615, lr: 0.08696408383337681\n",
      "epoch: 11000, acc: 0.727, loss: 0.607, lr: 0.08334027835652971\n",
      "epoch: 11500, acc: 0.727, loss: 0.599, lr: 0.08000640051204096\n",
      "epoch: 12000, acc: 0.730, loss: 0.592, lr: 0.07692899453804139\n",
      "epoch: 12500, acc: 0.730, loss: 0.585, lr: 0.07407956144899622\n",
      "epoch: 13000, acc: 0.737, loss: 0.579, lr: 0.07143367383384527\n",
      "epoch: 13500, acc: 0.737, loss: 0.574, lr: 0.06897027381198703\n",
      "epoch: 14000, acc: 0.737, loss: 0.569, lr: 0.06667111140742715\n",
      "epoch: 14500, acc: 0.743, loss: 0.564, lr: 0.06452029163171817\n",
      "epoch: 15000, acc: 0.743, loss: 0.559, lr: 0.06250390649415588\n",
      "epoch: 15500, acc: 0.750, loss: 0.555, lr: 0.060609733923268065\n",
      "epoch: 16000, acc: 0.753, loss: 0.551, lr: 0.058826989822930754\n",
      "epoch: 16500, acc: 0.757, loss: 0.547, lr: 0.05714612263557918\n",
      "epoch: 17000, acc: 0.753, loss: 0.544, lr: 0.055558642146785936\n",
      "epoch: 17500, acc: 0.753, loss: 0.540, lr: 0.05405697605275961\n",
      "epoch: 18000, acc: 0.757, loss: 0.537, lr: 0.05263434917627244\n",
      "epoch: 18500, acc: 0.757, loss: 0.534, lr: 0.051284681265705935\n",
      "epoch: 19000, acc: 0.757, loss: 0.531, lr: 0.050002500125006254\n",
      "epoch: 19500, acc: 0.760, loss: 0.528, lr: 0.048782867456949125\n",
      "epoch: 20000, acc: 0.763, loss: 0.525, lr: 0.047621315300728606\n",
      "epoch: 20500, acc: 0.763, loss: 0.522, lr: 0.046513791339132055\n",
      "epoch: 21000, acc: 0.760, loss: 0.519, lr: 0.045456611664166556\n",
      "epoch: 21500, acc: 0.763, loss: 0.517, lr: 0.044446419840881816\n",
      "epoch: 22000, acc: 0.767, loss: 0.514, lr: 0.043480151310926564\n",
      "epoch: 22500, acc: 0.770, loss: 0.512, lr: 0.04255500234052513\n",
      "epoch: 23000, acc: 0.770, loss: 0.510, lr: 0.04166840285011876\n",
      "epoch: 23500, acc: 0.773, loss: 0.508, lr: 0.04081799257112535\n",
      "epoch: 24000, acc: 0.773, loss: 0.505, lr: 0.040001600064002565\n",
      "epoch: 24500, acc: 0.777, loss: 0.503, lr: 0.03921722420487078\n",
      "epoch: 25000, acc: 0.777, loss: 0.501, lr: 0.03846301780837725\n",
      "epoch: 25500, acc: 0.777, loss: 0.499, lr: 0.03773727310464546\n",
      "epoch: 26000, acc: 0.780, loss: 0.497, lr: 0.03703840882995667\n",
      "epoch: 26500, acc: 0.780, loss: 0.496, lr: 0.03636495872577185\n",
      "epoch: 27000, acc: 0.780, loss: 0.494, lr: 0.035715561270045354\n",
      "epoch: 27500, acc: 0.780, loss: 0.492, lr: 0.03508895048949086\n",
      "epoch: 28000, acc: 0.780, loss: 0.490, lr: 0.03448394772233525\n",
      "epoch: 28500, acc: 0.780, loss: 0.489, lr: 0.033899454218787074\n",
      "epoch: 29000, acc: 0.783, loss: 0.487, lr: 0.03333444448148271\n",
      "epoch: 29500, acc: 0.783, loss: 0.486, lr: 0.03278796026099216\n",
      "epoch: 30000, acc: 0.783, loss: 0.484, lr: 0.032259105132423624\n",
      "epoch: 30500, acc: 0.787, loss: 0.483, lr: 0.031747039588558366\n",
      "epoch: 31000, acc: 0.790, loss: 0.481, lr: 0.03125097659301853\n",
      "epoch: 31500, acc: 0.797, loss: 0.480, lr: 0.030770177543924426\n",
      "epoch: 32000, acc: 0.797, loss: 0.478, lr: 0.030303948604503163\n",
      "epoch: 32500, acc: 0.793, loss: 0.477, lr: 0.029851637362309322\n",
      "epoch: 33000, acc: 0.793, loss: 0.475, lr: 0.029412629783228915\n",
      "epoch: 33500, acc: 0.793, loss: 0.474, lr: 0.0289863474303603\n",
      "epoch: 34000, acc: 0.793, loss: 0.473, lr: 0.028572244921283463\n",
      "epoch: 34500, acc: 0.793, loss: 0.471, lr: 0.02816980760021409\n",
      "epoch: 35000, acc: 0.793, loss: 0.470, lr: 0.027778549404150112\n",
      "epoch: 35500, acc: 0.793, loss: 0.469, lr: 0.027398010904408337\n",
      "epoch: 36000, acc: 0.793, loss: 0.468, lr: 0.027027757506959647\n",
      "epoch: 36500, acc: 0.797, loss: 0.466, lr: 0.026667377796741245\n",
      "epoch: 37000, acc: 0.797, loss: 0.465, lr: 0.026316482012684543\n",
      "epoch: 37500, acc: 0.800, loss: 0.463, lr: 0.025974700641575105\n",
      "epoch: 38000, acc: 0.803, loss: 0.462, lr: 0.02564168312008\n",
      "epoch: 38500, acc: 0.803, loss: 0.460, lr: 0.025317096635357857\n",
      "epoch: 39000, acc: 0.807, loss: 0.459, lr: 0.02500062501562539\n",
      "epoch: 39500, acc: 0.807, loss: 0.458, lr: 0.024691967702906242\n",
      "epoch: 40000, acc: 0.807, loss: 0.457, lr: 0.024390838800946363\n",
      "epoch: 40500, acc: 0.810, loss: 0.456, lr: 0.02409696619195643\n",
      "epoch: 41000, acc: 0.810, loss: 0.454, lr: 0.023810090716445628\n",
      "epoch: 41500, acc: 0.810, loss: 0.453, lr: 0.023529965410950846\n",
      "epoch: 42000, acc: 0.810, loss: 0.452, lr: 0.02325635479894881\n",
      "epoch: 42500, acc: 0.810, loss: 0.451, lr: 0.02298903423067197\n",
      "epoch: 43000, acc: 0.810, loss: 0.450, lr: 0.022727789267937906\n",
      "epoch: 43500, acc: 0.810, loss: 0.449, lr: 0.022472415110451918\n",
      "epoch: 44000, acc: 0.813, loss: 0.448, lr: 0.022222716060356897\n",
      "epoch: 44500, acc: 0.813, loss: 0.447, lr: 0.021978505022088396\n",
      "epoch: 45000, acc: 0.813, loss: 0.446, lr: 0.021739603034848582\n",
      "epoch: 45500, acc: 0.813, loss: 0.445, lr: 0.02150583883524377\n",
      "epoch: 46000, acc: 0.817, loss: 0.444, lr: 0.021277048447839314\n",
      "epoch: 46500, acc: 0.817, loss: 0.443, lr: 0.02105307480157477\n",
      "epoch: 47000, acc: 0.817, loss: 0.442, lr: 0.020833767370153543\n",
      "epoch: 47500, acc: 0.820, loss: 0.442, lr: 0.020618981834677003\n",
      "epoch: 48000, acc: 0.820, loss: 0.441, lr: 0.02040857976693402\n",
      "epoch: 48500, acc: 0.820, loss: 0.440, lr: 0.020202428331885493\n",
      "epoch: 49000, acc: 0.820, loss: 0.439, lr: 0.020000400008000158\n",
      "epoch: 49500, acc: 0.820, loss: 0.438, lr: 0.01980237232420444\n",
      "epoch: 50000, acc: 0.820, loss: 0.437, lr: 0.01960822761230612\n",
      "epoch: 50500, acc: 0.823, loss: 0.436, lr: 0.01941785277384027\n",
      "epoch: 51000, acc: 0.823, loss: 0.436, lr: 0.019231139060366546\n",
      "epoch: 51500, acc: 0.823, loss: 0.435, lr: 0.019047981866321263\n",
      "epoch: 52000, acc: 0.823, loss: 0.434, lr: 0.01886828053359497\n",
      "epoch: 52500, acc: 0.823, loss: 0.433, lr: 0.01869193816706854\n",
      "epoch: 53000, acc: 0.823, loss: 0.432, lr: 0.018518861460397416\n",
      "epoch: 53500, acc: 0.823, loss: 0.432, lr: 0.018348960531385895\n",
      "epoch: 54000, acc: 0.823, loss: 0.431, lr: 0.018182148766341204\n",
      "epoch: 54500, acc: 0.823, loss: 0.430, lr: 0.018018342672840953\n",
      "epoch: 55000, acc: 0.827, loss: 0.429, lr: 0.01785746174038822\n",
      "epoch: 55500, acc: 0.827, loss: 0.429, lr: 0.017699428308465635\n",
      "epoch: 56000, acc: 0.827, loss: 0.428, lr: 0.017544167441534062\n",
      "epoch: 56500, acc: 0.827, loss: 0.427, lr: 0.017391606810553226\n",
      "epoch: 57000, acc: 0.830, loss: 0.426, lr: 0.0172416765806307\n",
      "epoch: 57500, acc: 0.830, loss: 0.426, lr: 0.017094309304432554\n",
      "epoch: 58000, acc: 0.830, loss: 0.425, lr: 0.016949439821013913\n",
      "epoch: 58500, acc: 0.830, loss: 0.424, lr: 0.016807005159750584\n",
      "epoch: 59000, acc: 0.830, loss: 0.424, lr: 0.01666694444907415\n",
      "epoch: 59500, acc: 0.830, loss: 0.423, lr: 0.016529198829732722\n",
      "epoch: 60000, acc: 0.830, loss: 0.422, lr: 0.016393711372317578\n",
      "epoch: 60500, acc: 0.830, loss: 0.422, lr: 0.01626042699881299\n",
      "epoch: 61000, acc: 0.830, loss: 0.421, lr: 0.016129292407942062\n",
      "epoch: 61500, acc: 0.830, loss: 0.420, lr: 0.016000256004096065\n",
      "epoch: 62000, acc: 0.830, loss: 0.420, lr: 0.01587326782964809\n",
      "epoch: 62500, acc: 0.830, loss: 0.419, lr: 0.015748279500464573\n",
      "epoch: 63000, acc: 0.830, loss: 0.418, lr: 0.015625244144439755\n",
      "epoch: 63500, acc: 0.833, loss: 0.418, lr: 0.015504116342889038\n",
      "epoch: 64000, acc: 0.833, loss: 0.417, lr: 0.015384852074647303\n",
      "epoch: 64500, acc: 0.833, loss: 0.417, lr: 0.015267408662727676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65000, acc: 0.837, loss: 0.416, lr: 0.015151744723404902\n",
      "epoch: 65500, acc: 0.837, loss: 0.415, lr: 0.015037820117595755\n",
      "epoch: 66000, acc: 0.837, loss: 0.415, lr: 0.014925595904416484\n",
      "epoch: 66500, acc: 0.840, loss: 0.414, lr: 0.014815034296804398\n",
      "epoch: 67000, acc: 0.840, loss: 0.414, lr: 0.01470609861909734\n",
      "epoch: 67500, acc: 0.837, loss: 0.413, lr: 0.014598753266471044\n",
      "epoch: 68000, acc: 0.840, loss: 0.412, lr: 0.01449296366614009\n",
      "epoch: 68500, acc: 0.840, loss: 0.412, lr: 0.014388696240233673\n",
      "epoch: 69000, acc: 0.843, loss: 0.411, lr: 0.014285918370262433\n",
      "epoch: 69500, acc: 0.843, loss: 0.411, lr: 0.01418459836309735\n",
      "epoch: 70000, acc: 0.847, loss: 0.410, lr: 0.014084705418386176\n",
      "epoch: 70500, acc: 0.847, loss: 0.410, lr: 0.013986209597337027\n",
      "epoch: 71000, acc: 0.847, loss: 0.409, lr: 0.013889081792802679\n",
      "epoch: 71500, acc: 0.847, loss: 0.409, lr: 0.013793293700602768\n",
      "epoch: 72000, acc: 0.847, loss: 0.408, lr: 0.013698817792024549\n",
      "epoch: 72500, acc: 0.847, loss: 0.408, lr: 0.013605627287446088\n",
      "epoch: 73000, acc: 0.847, loss: 0.407, lr: 0.0135136961310288\n",
      "epoch: 73500, acc: 0.850, loss: 0.406, lr: 0.013422998966429081\n",
      "epoch: 74000, acc: 0.850, loss: 0.406, lr: 0.013333511113481513\n",
      "epoch: 74500, acc: 0.850, loss: 0.405, lr: 0.013245208545808554\n",
      "epoch: 75000, acc: 0.850, loss: 0.405, lr: 0.013158067869314071\n",
      "epoch: 75500, acc: 0.850, loss: 0.404, lr: 0.013072066301520283\n",
      "epoch: 76000, acc: 0.850, loss: 0.404, lr: 0.012987181651709763\n",
      "epoch: 76500, acc: 0.850, loss: 0.403, lr: 0.012903392301836153\n",
      "epoch: 77000, acc: 0.850, loss: 0.403, lr: 0.01282067718816908\n",
      "epoch: 77500, acc: 0.850, loss: 0.402, lr: 0.012739015783640556\n",
      "epoch: 78000, acc: 0.850, loss: 0.402, lr: 0.012658388080861784\n",
      "epoch: 78500, acc: 0.850, loss: 0.402, lr: 0.012578774575780828\n",
      "epoch: 79000, acc: 0.850, loss: 0.401, lr: 0.01250015625195315\n",
      "epoch: 79500, acc: 0.850, loss: 0.401, lr: 0.012422514565398328\n",
      "epoch: 80000, acc: 0.850, loss: 0.400, lr: 0.012345831430017655\n",
      "epoch: 80500, acc: 0.850, loss: 0.400, lr: 0.012270089203548511\n",
      "epoch: 81000, acc: 0.853, loss: 0.399, lr: 0.01219527067403261\n",
      "epoch: 81500, acc: 0.853, loss: 0.399, lr: 0.012121359046776326\n",
      "epoch: 82000, acc: 0.853, loss: 0.398, lr: 0.012048337931782312\n",
      "epoch: 82500, acc: 0.853, loss: 0.398, lr: 0.011976191331632715\n",
      "epoch: 83000, acc: 0.853, loss: 0.397, lr: 0.011904903629805117\n",
      "epoch: 83500, acc: 0.857, loss: 0.397, lr: 0.011834459579403307\n",
      "epoch: 84000, acc: 0.857, loss: 0.397, lr: 0.011764844292285793\n",
      "epoch: 84500, acc: 0.857, loss: 0.396, lr: 0.011696043228575773\n",
      "epoch: 85000, acc: 0.857, loss: 0.396, lr: 0.011628042186537053\n",
      "epoch: 85500, acc: 0.857, loss: 0.395, lr: 0.011560827292801074\n",
      "epoch: 86000, acc: 0.857, loss: 0.395, lr: 0.011494384992930954\n",
      "epoch: 86500, acc: 0.857, loss: 0.394, lr: 0.011428702042309056\n",
      "epoch: 87000, acc: 0.857, loss: 0.394, lr: 0.011363765497335197\n",
      "epoch: 87500, acc: 0.857, loss: 0.394, lr: 0.011299562706923243\n",
      "epoch: 88000, acc: 0.857, loss: 0.393, lr: 0.011236081304284318\n",
      "epoch: 88500, acc: 0.857, loss: 0.393, lr: 0.011173309198985465\n",
      "epoch: 89000, acc: 0.860, loss: 0.392, lr: 0.011111234569272993\n",
      "epoch: 89500, acc: 0.860, loss: 0.392, lr: 0.011049845854650329\n",
      "epoch: 90000, acc: 0.860, loss: 0.392, lr: 0.010989131748700537\n",
      "epoch: 90500, acc: 0.860, loss: 0.391, lr: 0.010929081192144178\n",
      "epoch: 91000, acc: 0.860, loss: 0.391, lr: 0.010869683366123546\n",
      "epoch: 91500, acc: 0.860, loss: 0.390, lr: 0.010810927685704711\n",
      "epoch: 92000, acc: 0.860, loss: 0.390, lr: 0.010752803793589179\n",
      "epoch: 92500, acc: 0.860, loss: 0.390, lr: 0.010695301554027316\n",
      "epoch: 93000, acc: 0.860, loss: 0.389, lr: 0.010638411046926031\n",
      "epoch: 93500, acc: 0.860, loss: 0.389, lr: 0.010582122562143515\n",
      "epoch: 94000, acc: 0.860, loss: 0.389, lr: 0.010526426593964147\n",
      "epoch: 94500, acc: 0.860, loss: 0.388, lr: 0.010471313835746971\n",
      "epoch: 95000, acc: 0.860, loss: 0.388, lr: 0.010416775174741403\n",
      "epoch: 95500, acc: 0.860, loss: 0.387, lr: 0.010362801687064115\n",
      "epoch: 96000, acc: 0.860, loss: 0.387, lr: 0.010309384632831266\n",
      "epoch: 96500, acc: 0.863, loss: 0.387, lr: 0.010256515451440528\n",
      "epoch: 97000, acc: 0.863, loss: 0.386, lr: 0.010204185756997521\n",
      "epoch: 97500, acc: 0.863, loss: 0.386, lr: 0.010152387333881564\n",
      "epoch: 98000, acc: 0.863, loss: 0.386, lr: 0.010101112132445782\n",
      "epoch: 98500, acc: 0.863, loss: 0.385, lr: 0.010050352264846883\n",
      "epoch: 99000, acc: 0.857, loss: 0.383, lr: 0.01000010000100001\n",
      "epoch: 99500, acc: 0.860, loss: 0.382, lr: 0.009950347764654375\n",
      "epoch: 100000, acc: 0.860, loss: 0.382, lr: 0.009901088129585442\n"
     ]
    }
   ],
   "source": [
    "# Create spiral data\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "# Initiate instances of NN classes. We need more neurons in the hidden layer to make it more accurate.\n",
    "hidden_layer = LayerDense(2, 50)\n",
    "ReLU = ActivationRelU()\n",
    "output_layer = LayerDense(50, 3)\n",
    "softmax_cost = ActivationSoftmaxCost()\n",
    "\n",
    "# Create optimiser instance object\n",
    "sgd = Optimizer_SGD(decay = 1e-3, momentum = 0.9)\n",
    "\n",
    "# Train in epochs. 40001 iterations.\n",
    "for epoch in range(100001):\n",
    "    # Forward propagation\n",
    "    hidden_layer.forward(X)\n",
    "    ReLU.forward(hidden_layer.output)\n",
    "    output_layer.forward(ReLU.output)\n",
    "    \n",
    "    # Calculate error\n",
    "    cost = softmax_cost.forward(output_layer.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of softmax and y\n",
    "    predictions = np.argmax(softmax_cost.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    # Print statistics per set of epochs\n",
    "    if not epoch % 500:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {cost:.3f}, ' +\n",
    "              f'lr: {sgd.current_learning_rate}')\n",
    "        \n",
    "    # Back propagation \n",
    "    softmax_cost.backward(softmax_cost.output, y)\n",
    "    output_layer.backward(softmax_cost.dinputs)\n",
    "    ReLU.backward(output_layer.dinputs)\n",
    "    hidden_layer.backward(ReLU.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    sgd.pre_update_params()\n",
    "    sgd.update_params(hidden_layer)\n",
    "    sgd.update_params(output_layer)\n",
    "    sgd.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network achieved 90 % accuracy after 100000 epochs. To achieve a greater accuracy, further hyper parameters will need to be added such L1 and L2 regularisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
